---
title: "Downloading and Preparing NOAA OISST Data"
author: "Robert W Schlegel and AJ Smit"
date: "`r Sys.Date()`"
description: "This vignette demonstrates how to download NOAA OISST data and prepare them for the detection of marine heatwaves."
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Downloading and Preparing NOAA OISST Data}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 3, fig.align = 'centre',
                      echo = TRUE, warning = FALSE, message = FALSE,
                      eval = FALSE, tidy = FALSE)
```

## Overview

In this vignette we will see how to retrieve and prepare [Reynolds optimally interpolated sea surface temperature](https://journals.ametsoc.org/doi/full/10.1175/2007JCLI1824.1) (OISST) data for calculating marine heatwaves (MHWs). The OISST product is a global 1/4 degree gridded dataset of Advanced Very High Resolution Radiometer (AVHRR) derived SSTs at a daily resolution, starting on 1 September 1981. The source of the data is currently the [NOAA NCDC](https://www.ncdc.noaa.gov/oisst). 

Each daily global file is available in netCDF format and is roughly 1.6 MB. This means that one full year of global data will be roughly 600 MB, and the full dataset roughly 25 GB. This is however when the data are very compressed. If we were to attempt to load the entire uncompressed dataset into our memory at once it would take more than 200 GB of RAM. That is well beyond the scope of any current laptop so in this vignette we will see how to download the full OISST dataset before then seeing how we can load only a subset of the data into the R environment for use with further analyses.

## Setup

For this vignette we will be accessing the NOAA OISST dataset indexed [here](https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/). One may download the data there manually by clicking on each file individually. But programming languages like R are designed to prevent us from needing to experience that sort of anguish. Below we will load the libraries we need in order to have R download all of the files that we may need. If any of the lines of code in the following chunk do not run it means that we will need to first install that package. Uncomment the line of code that would install the problem package and run it before trying to load the library again. 

```{r setup}
# The two packages we will need
  # NB: The packages only need to be installed from GitHub once
# install.packages("dplyr")
# install.packages("lubridate")
# install.packages("ggplot2")
# install.packages("stringr")
# install.packages("RCurl")
# install.packages("XML")
# install.packages("tidync")
# install.packages("doParallel")
# install.packages("ncdf4")
# install.packages("plyr") # Note that this library is not explicitly loaded

# Load the packages once they have been downloaded and installed
# The packages we will use
library(dplyr) # A staple for most modern data management in R
library(lubridate) # Useful functions for dealing with dates
# library(stringr) # Used for easy manipulation of character strings
library(ggplot2) # The preferred library for data visualisation
# library(RCurl) # For helping R to make sense of URLs for web hosted data
# library(XML) # For reading the HTML tables created by RCurl
library(tidync) # For easily dealing with NetCDF data
library(ncdf4) # For creating netCDF files
library(doParallel) # For parallel processing
```

With our packages loaded we may now begin downloading and preparing our data for further use. We will break this step down into some key parts in order to keep this process as clear as possible. Before we begin I need to stress that this is a very direct and unrestricted method for accessing these data and I urge responsibility in only downloading as much data as are necessary. Please do not download the entire dataset unless you have a specific need for it, such as [detecting marine heatwaves (MHWs) in gridded data](https://robwschlegel.github.io/heatwaveR/articles/gridded_event_detection.html).

## File Information

The first step in downloading the data is to tell you computer where they are. There is an automated way to do this but it requires a couple of additional packages and we aim to keep this vignette as simple and direct as possible. For our purposes today we will manually create the URLs of the files we want to download.

```{r NOAA-info}
# First we tell R where the data are on the interwebs
OISST_base_url <- "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/"
# Note that one may go to this URL in any web browser to manually inspect the files

# Now we create a data.frame that contains all of the dates we want to download
  # NB: In order to change the dates download changes the dates in the following line
OISST_dates <- data.frame(t = seq(as.Date("2019-01-01"), as.Date("2019-01-31"), by = "day"))

# To finish up this step we add some text to those dates so they match the OISST file names
OISST_files <- OISST_dates %>% 
  mutate(t_day = gsub("-", "", t),
         t_month = substr(t_day, 1, 6),
         t_year = year(t),
         file_name = paste0(OISST_base_url, t_month, "/", "oisst-avhrr-v02r01.", t_day ,".nc"))
```

## Downloading data

Now that we have a dataframe that contains all of the URLs for the files we want to download let's create a function that will crawl through those URLs and download the files for us.

```{r NOAA-dl}
# This function will go about downloading each day of data as a NetCDF file
# Note that this will download files into a 'data/OISST' folder in the root directory
  # If this folder does not exist it will create it
  # If it does not automatically create the folder it will need to be done manually
  # The folder that is created must be a new folder with no other files in it
  # A possible bug with netCDF files in R is they won't load correctly from 
  # existing folders with other file types in them
# This function will also check if the file has been previously downloaded
  # If it has it will not download it again
OISST_url_daily_dl <- function(target_URL){
  dir.create("~/data/OISST_daily", showWarnings = F)
  file_name <- paste0("~/data/OISST_daily/",sapply(strsplit(target_URL, split = "/"), "[[", 10))
  if(!file.exists(file_name)) download.file(url = target_URL, method = "libcurl", destfile = file_name)
}

# The more cores used, the faster the data may be downloaded
  # It is best practice to not use all of the cores on one's machine
  # The laptop on which I am running this code has 8 cores, so I use 7 here
doParallel::registerDoParallel(cores = 7)

# And with that we are clear for take off
system.time(plyr::l_ply(OISST_files$file_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds

# In roughly 15 seconds a user may have a full month of global data downloaded
# This scales well into years and decades, and is much faster with more cores
# Download speeds will also depend on the speed of the users internet connection
```

## Loading data

The following code chunk contains the function we may use to load and prepare our OISST data for further use in R.

```{r NOAA-load}
# This function will load and subset daily data into one data.frame
# Note that the subsetting by lon/lat is done before the data are loaded
# This means it will use much less RAM and is viable for use on most laptops
# Assuming one's study area is not too large
OISST_load <- function(file_name, lon1, lon2, lat1, lat2){
      OISST_dat <- tidync(file_name) %>%
        hyper_filter(lon = between(lon, lon1, lon2),
                     lat = between(lat, lat1, lat2)) %>% 
        hyper_tibble() %>% 
        select(lon, lat, time, sst) %>% 
        dplyr::rename(t = time, temp = sst) %>% 
        mutate(t = as.Date(t, origin = "1978-01-01"))
      return(OISST_dat)
}

# Locate the files that will be loaded
OISST_files <- dir("~/data/OISST_daily", full.names = T)

# Load the data in parallel
OISST_dat <- plyr::ldply(.data = OISST_files, .fun = OISST_load, .parallel = T,
                         lon1 = 270, lon2 = 320, lat1 = 30, lat2 = 50)

# It should only take a few seconds to load one month of data depending on the size of the lon/lat extent chosen
```

In the code chunk above I have chosen the spatial extent of longitude 270 to 320 and latitude 30 to 50. This a window over the Atlantic Coast of North America. One may simply change the lon/lat values above as necessary to match the desired study area. The function also re-labels the 'time' column as 't', and the 'sst' column as 'temp'. We do this now so that they match the default column names that are expected for calculating MHWs so we won't have to do any extra work later on.

One must note here that depending on the RAM available on one's machine, it may not be possible to handle all of the data loaded at once if they are very large (e.g. > 5 GB). The discussion on the limitations of the R language due to its dependence on virtual memory is beyond the scope of this vignette, but if one limits one's lon/lat range to no more than several degrees on lon/lat at once that should be fine for almost all machines. Were one to try to load 30+ years of the data for the whole Indian Ocean, for example, that may cause issues if being run on a laptop or computer of a similar power.

## Visualising data

It is always good to visualise data early and often in any workflow. The code pipeline below shows how we can visualise a day of data from those we've loaded.

```{r NOAA-visual}
OISST_dat %>% 
  filter(t == "2019-01-01") %>% 
  ggplot(aes(x = lon, y = lat)) +
  geom_tile(aes(fill = temp)) +
  scale_fill_viridis_c() +
  coord_quickmap(expand = F) +
  labs(x = NULL, y = NULL, fill = "SST (Â°C)") +
  theme(legend.position = "bottom")
```

## Compiling data

If one only needed to download a few months or a year or two of data it won't be necessary to compile the data further and the rest of this vignette can be skipped. If however one intends to download 30+ years of data in order to calculate MHWs or some other sort of time series analysis it will be incredibly useful to compile the daily netCDF files into monthly or annual files instead. There are other software tools that one can use to do this, but this is an R package, so we are going to do it in R!

```{r netCDF-create}
# This function can be used to compile daily netCDF files into monthly or annual files
ncdf_compile <- function(file_names){
  
  # Lad the data
  ncdf_dat <- plyr::ldply(.data = file_names, .fun = OISST_load, .parallel = T,
                          lon1 = 0, lon2 = 360, lat1 = -90, lat2 = 90)
  
  # Detect if more than one month of data is present
  ncdf_dates <- ncdf_dat %>% 
    dplyr::select(t) %>% 
    unique() %>% 
    mutate(t_day = gsub("-", "", t),
           t_month = substr(t_day, 1, 6),
           t_year = year(t))
  
  # Create file name based on data
  # NB: If this folder is not created the user must create it manually
    # Or change this directory to a desired location
  if(length(unique(ncdf_dates$t_month)) == 1){
    dir.create("~/data/OISST_monthly", showWarnings = F)
    ncdf_name <- paste0("~/data/OISST_monthly/oisst-avhrr-v02r01.", 
                        unique(ncdf_dates$t_month), ".nc")
  } else if(length(unique(ncdf_dates$t_year)) == 1){
    dir.create("~/data/OISST_annual", showWarnings = F)
    ncdf_name <- paste0("~/data/OISST_annual/oisst-avhrr-v02r01.", 
                        unique(ncdf_dates$t_year), ".nc")
  } else if(length(unique(ncdf_dates$t_year)) > 1){
        dir.create("~/data/OISST_annual", showWarnings = F)
    ncdf_name <- paste0("~/data/OISST_annual/oisst-avhrr-v02r01.", 
                        min(unique(ncdf_dates$t_year)), "_",
                        max(unique(ncdf_dates$t_year)), ".nc")
  }
  
  # Establish the lon, lat, and t values
  ncdf_lon <- unique(ncdf_dat$lon)
  ncdf_lat <- unique(ncdf_dat$lat)
  ncdf_t <- unique(ncdf_dat$t)
  
  # We then create an array from the data
  temp_array <- array(data = ncdf_dat$temp, 
                      dim = c(length(ncdf_lat), length(ncdf_lon), length(ncdf_t)),
                      dimnames = list(lat = ncdf_lat, lon = ncdf_lon, t = ncdf_t))

  # And assign values to each cell
  # dimnames(temp_array) <- list(lat = OISST_lat,
  #                              lon = OISST_lon,
  #                              t = OISST_t)
  
  # Define dimensions
  lon_dim <- ncdim_def("lon", "degrees_east", as.double(ncdf_lon)) 
  lat_dim <- ncdim_def("lat", "degrees_north", as.double(ncdf_lat)) 
  t_dim <- ncdim_def("t", "days since 1978-01-01 12:00:00", as.double(ncdf_t))

  # Define temperature variable
  temp_def <- ncvar_def(name = "temp", units = "deg_C",
                        dim = list(lon_dim, lat_dim, t_dim), missval = 9999, 
                        longname = "Sea surface temperature", prec = "single")

  # Create the shell of the netCDF file
  ncdf_out <- nc_create(filename = ncdf_name, vars = list(temp_def), force_v4 = T)

  # Add the SST data
  ncvar_put(nc = ncdf_out, varid = temp_def, vals = temp_array)

  # Add the dimension info
  ncatt_put(nc = ncdf_out, varid = "lon", attname = "axis", attval = "X")
  ncatt_put(nc = ncdf_out, varid = "lat", attname = "axis", attval = "Y")
  ncatt_put(nc = ncdf_out, varid = "t", attname = "axis", attval = "T")

  # NB: It is possible to add much more information to a netCDF file
    # Please see the help file for ncatt_put for more info

  # Write the data to disk
  nc_close(ncdf_out)
}

# Compile the one month of data we've downloaded
ncdf_compile(OISST_files)
```

And now we check that our netCDF file was created correctly.

```{r netCDF-check}

```


In the next vignette we will see how to [detect MHWs in gridded data](https://robwschlegel.github.io/heatwaveR/articles/gridded_event_detection.html).

