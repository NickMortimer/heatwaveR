---
title: "Detecting Events in Gridded Data"
author: "Robert W Schlegel and AJ Smit"
date: "`r Sys.Date()`"
description: "This vignette demonstrates how to deetect marine heatwaves (MHWs) in dataframes of gridded data in which each pixel is a separate time series."
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Detecting Events in Gridded Data}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 4, fig.align = 'center',
                      echo = TRUE, warning = FALSE, message = FALSE, 
                      eval = FALSE, tidy = FALSE)
```

## Overview

Because of human-induced climate change, we anticipate that extreme events will occur more frequently and that they will become greater in intensity. Here we investigate this hypothesis by using gridded SST data, which is the only way that we can assess if this trend is unfolding across large ocean regions. Using the gridded [0.25 degree Reynolds OISST](http://dx.doi.org/10.1175/2007JCLI1824.1), we will detect marine heatwaves (MHWs) around South Africa by applying the `detect_event()` function pixel-by-pixel to the data we downloaded in [the previous vignette](https://robwschlegel.github.io/heatwaveR/articles/OISST_preparation.html). After detecting the events, we will fit a generalised linear model (GLM) to each pixel to calculate rates of change in some MHW metrics, and then plot the estimated trends. 

```{r load-pkg}
library(dplyr) # For basic data manipulation
library(ggplot2) # For visualising data
library(heatwaveR) # For detecting MHWs
library(tidync) # For easily dealing with NetCDF data
library(doParallel) # For parallel processing
```

## Gridded data

In the previous vignette we saw how to [download and prepare OISST data](https://robwschlegel.github.io/heatwaveR/articles/OISST_preparation.html). In this vignette we will use the data we downloaded for our example on how to detect MHWs in gridded data. Note that because we are going to be detecting MHWs we will want at least 30 years of data. The code in the previous vignette can be used to download these data with only minor alterations. It would be preferable to use an ERDDAP server to download only the data necessary, but as of this writing (June 6th, 2020) NOAA OISST v2.1 data are not available on any ERDDAP servers. So we will need to make a plan in order to deal with these large global data files while only using a laptop.

```{r load-data}
# Load one file
load_region <- function(file_name){
      OISST_dat <- tidync(file_name) %>%
        hyper_filter(lon = between(lon, 15, 21),
                     lat = between(lat, -40, -35)) %>% 
        hyper_tibble() %>% 
        select(lon, lat, time, sst) %>% 
        dplyr::rename(t = time, temp = sst) %>% 
        mutate(t = as.Date(t, origin = "1978-01-01"))
      return(OISST_dat)
}

# Locate the files that will be loaded
OISST_files <- dir("~/data/OISST", full.names = T)

# NB: One should never use ALL available cores, save at least 1 for other essential tasks
# The computer I'm writing this vignette on has 8 cores, so I use 6 here
registerDoParallel(cores = 7)

# Load the data in parallel
# Note that it will take quite a while to load 30 years of data
# because each day is saved as a separate file
# On 6 cores it took more than 30 minutes
# Run this as only a few years at a time in case of errors
# It may be necessary to reduce the number of files being loaded at once depending on one's RAM
# It's also a safer choice to save these files as we go
OISST1 <- plyr::ldply(.data = OISST_files[1:1000], .fun = load_region, .parallel = T)
saveRDS(OISST1, "~/data/OISST1_agulhas_region.Rda")
OISST2 <- plyr::ldply(.data = OISST_files[1001:2000], .fun = load_region, .parallel = T)
saveRDS(OISST2, "~/data/OISST2_agulhas_region.Rda")
OISST3 <- plyr::ldply(.data = OISST_files[2001:3000], .fun = load_region, .parallel = T)
saveRDS(OISST3, "~/data/OISST3_agulhas_region.Rda")
OISST4 <- plyr::ldply(.data = OISST_files[3001:4000], .fun = load_region, .parallel = T)
saveRDS(OISST4, "~/data/OISST4_agulhas_region.Rda")
OISST5 <- plyr::ldply(.data = OISST_files[4001:5000], .fun = load_region, .parallel = T)
saveRDS(OISST5, "~/data/OISST5_agulhas_region.Rda")
OISST6 <- plyr::ldply(.data = OISST_files[5001:6000], .fun = load_region, .parallel = T)
saveRDS(OISST6, "~/data/OISST6_agulhas_region.Rda")
OISST7 <- plyr::ldply(.data = OISST_files[6001:7000], .fun = load_region, .parallel = T)
saveRDS(OISST7, "~/data/OISST7_agulhas_region.Rda")
OISST8 <- plyr::ldply(.data = OISST_files[7001:8000], .fun = load_region, .parallel = T)
saveRDS(OISST8, "~/data/OISST8_agulhas_region.Rda")
OISST9 <- plyr::ldply(.data = OISST_files[8001:9000], .fun = load_region, .parallel = T)
saveRDS(OISST9, "~/data/OISST9_agulhas_region.Rda")
OISST10 <- plyr::ldply(.data = OISST_files[9001:10000], .fun = load_region, .parallel = T)
saveRDS(OISST10, "~/data/OISST10_agulhas_region.Rda")
OISST11 <- plyr::ldply(.data = OISST_files[10001:11000], .fun = load_region, .parallel = T)
saveRDS(OISST11, "~/data/OISST11_agulhas_region.Rda")
OISST12 <- plyr::ldply(.data = OISST_files[11001:12000], .fun = load_region, .parallel = T)
saveRDS(OISST12, "~/data/OISST12_agulhas_region.Rda")
OISST13 <- plyr::ldply(.data = OISST_files[12001:13000], .fun = load_region, .parallel = T)
saveRDS(OISST13, "~/data/OISST13_agulhas_region.Rda")
OISST14 <- plyr::ldply(.data = OISST_files[13001:13879], .fun = load_region, .parallel = T)
saveRDS(OISST14, "~/data/OISST14_agulhas_region.Rda")

# Then combine them
OISST <- rbind(OISST1, OISST2, OISST3, OISST4, OISST5, OISST6, OISST7,
               OISST8, OISST9, OISST10, OISST11, OISST12, OISST13, OISST14)

# Save our new file for faster loading if we need it again
saveRDS(OISST, "~/data/OISST_agulhas_region.Rda")

# Remove the smaller files and clean up
rm(OISST1, OISST2, OISST3, OISST4, OISST5, OISST6, OISST7,
   OISST8, OISST9, OISST10, OISST11, OISST12, OISST13, OISST14)
gc()
```

This is obviously not great if it takes us over 30 minutes to load the data and dozens of lines of repeated code. Ideally one would be able to access only the necessary data via an ERDDAP server but this is not currently possible. When NOAA hosts the v2.1 OISST data on an ERDDAP server I'll upload a new vignette that shows how to perform the same task as above in a much shorter and easier way.

## Event detection

When we want to make the same calculation across multiple groups of data within one dataframe we have many good options available to us. The first is to make use of the `map()` suite of functions found in the __`purrr`__ package, and now implemented in __`dplyr`__. This is a very fast __`tidyverse`__ friendly approach to splitting up tasks. Another good option is to go back in time a bit and use the `ddply()` function from the __`plyr`__ package. This is arguably a better approach as it allows us to very easily use multiple cores to detect the MHWs. The problem with this approach is that one must _never_ load the __`plyr`__ library directly as it has some fundamental inconsistencies with __`dplyr`__ and other packages in the __`tidyverse`__. In a previous version of this vignette I provided multiple approaches for calculating MHWs but I've found over time that the functionality that is being added to __`dplyr`__ and it's siblings is consistently slower and more convoluted than the original __`plyr`__ approach. For that reason we will use only that method here.

It is a little clumsy to use multiple functions at once with `ddply()` so we will combine the calculations we want to make into one wrapper function first.

```{r detect-func}
event_only <- function(df){
  # First calculate the climatologies
  clim <- ts2clm(data = df, climatologyPeriod = c("1982-01-01", "2011-01-01"))
  # Then the events
  event <- detect_event(data = clim)
  # Return only the event metric dataframe of results
  return(event$event)
}
```

With that convenience function loaded into our environment we may now use `ddply()` in just one line of code to detect all of the MHWs in our study area.

```{r detect-plyr}
system.time(
MHW_result <- plyr::ddply(.data = OISST, .variables = c("lon", "lat"), .fun = event_only, .parallel = TRUE)
) # 30 seconds
```

The __`plyr`__ method took 30 seconds using seven cores. This method is not seven times faster than if we used a single core because when using multiple cores there is a certain amount of loss in efficiency due to the computer needing to remember which results are meant to go where so that it can stitch everything back together again for you. This takes very little memory, but over large jobs it can start to become problematic. Occasionally 'slippage' can occur as well where an entire task can be forgotten. This is very rare but does happen. This is partly what makes __`dplyr`__ a viable option as it does not have this problem. To further maximise efficiency I would recommend writing out this full workflow in a stand-alone script and then running it using `source()` directly from an R terminal.

## Trend detection

With our MHW detected we will now look at how to fit some GLMs to the results in order to determine long-term trends in MHW occurrence.

Up first we see how to calculate the number of events that occurred per pixel.

```{r event-tally}
# summarise the number of unique longitude, latitude and year combination:
event_freq <- MHW_result %>% 
  mutate(year = lubridate::year(date_start)) %>% 
  group_by(lon, lat, year) %>% 
  summarise(n = n())
head(event_freq)

# create complete grid for merging with:
sst_grid <- OISST %>% 
  select(lon, lat, t) %>% 
  mutate(t = lubridate::year(t)) %>% 
  dplyr::rename(year = t) %>% 
  distinct()

# and merge:
OISST_n <- left_join(sst_grid, event_freq, by = c("lon", "lat", "year")) %>% 
  mutate(n = ifelse(is.na(n), 0, n))
```

Then we specify the particulars of the GLM we are going to use.

```{r trend-fun}
lin_fun <- function(ev) {
  mod1 <- glm(n ~ year, family = poisson(link = "log"), data = ev)
  # extract slope coefficient and its p-value
  tr <- data.frame(slope = summary(mod1)$coefficients[2,1],
                   p = summary(mod1)$coefficients[2,4])
  return(tr)
}
```

Lastly we make the calculations.

```{r apply-trend-fun-plyr}
OISST_nTrend <- plyr::ddply(OISST_n, c("lon", "lat"), lin_fun, .parallel = T)
OISST_nTrend$pval <- cut(OISST_nTrend$p, breaks = c(0, 0.001, 0.01, 0.05, 1))
head(OISST_nTrend)
```

## Visualising the results

Let's finish this vignette by visualising the long-term trends in the annual occurrence of MHWs per pixel in the chosen study area. First we will grab the base global map from the __`maps`__ package.

```{r prep-geography}
# The base map
map_base <- ggplot2::fortify(maps::map(fill = TRUE, plot = FALSE)) %>% 
  dplyr::rename(lon = long)
```

Then we will create two maps that we will stick together using __`ggpubr`__. The first map will show the slope of the count of events detected per year over time as shades of red, and the second map will show the significance (_p_-value) of these trends in shades of grey.

```{r the-figure}
map_slope <- ggplot(OISST_nTrend, aes(x = lon, y = lat)) +
  geom_rect(size = 0.2, fill = NA,
       aes(xmin = lon - 0.1, xmax = lon + 0.1, ymin = lat - 0.1, ymax = lat + 0.1,
           colour = pval)) +
  geom_raster(aes(fill = slope), interpolate = FALSE, alpha = 0.9) +
  scale_fill_gradient2(name = "count/year (slope)", high = "red", mid = "white",
                       low = "darkblue", midpoint = 0,
                       guide = guide_colourbar(direction = "horizontal",
                                               title.position = "top")) +
  scale_colour_manual(breaks = c("(0,0.001]", "(0.001,0.01]", "(0.01,0.05]", "(0.05,1]"),
                      values = c("firebrick1", "firebrick2", "firebrick3", "white"),
                      name = "p-value", guide = FALSE) +
  geom_polygon(data = map_base, aes(group = group), 
               colour = NA, fill = "grey80") +
  coord_fixed(ratio = 1, xlim = c(13.0, 23.0), ylim = c(-33, -42), expand = TRUE) +
  labs(x = "", y = "") +
  theme_bw() +
  theme(legend.position = "bottom")

map_p <- ggplot(OISST_nTrend, aes(x = lon, y = lat)) +
  geom_raster(aes(fill = pval), interpolate = FALSE) +
  scale_fill_manual(breaks = c("(0,0.001]", "(0.001,0.01]", "(0.01,0.05]",
                               "(0.05,0.1]", "(0.1,0.5]", "(0.5,1]"),
                    values = c("black", "grey20", "grey40",
                               "grey80", "grey90", "white"),
                    name = "p-value",
                    guide = guide_legend(direction = "horizontal",
                                               title.position = "top")) +
  geom_polygon(data = map_base, aes(group = group), 
               colour = NA, fill = "grey80") +
  coord_fixed(ratio = 1, xlim = c(13.0, 23.0), ylim = c(-33, -42), expand = TRUE) +
  labs(x = "", y = "") +
  theme_bw() +
  theme(legend.position = "bottom")

map_both <- ggpubr::ggarrange(map_slope, map_p, align = "hv")
map_both
```

From the figure above we may see that the entire study area shows significant (_p_<= 0.05) increases in the count of MHWs per year. This is generally the case for the entire globe. Not shown here is the significant increase in the intensity of MHWs as well.
